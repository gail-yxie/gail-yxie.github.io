[{"authors":["admin"],"categories":null,"content":"I am currently a fifth-year CSEM PhD student at UT Austin, advised by Rachel Ward. My research focuses on understanding optimization and generalization in deep learning. In particular, I am interested in adaptive stochastic gradient descent algorithms, generalization of overparameterized systems, sparse random feature models, and robust learning.\nBefore joining UT Austin, I received my bachelor’s degree in Mathematics and Applied Mathematics from Zhejiang University, China. And I was an exachange student at Lund University, Sweden, during Fall 2015.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently a fifth-year CSEM PhD student at UT Austin, advised by Rachel Ward. My research focuses on understanding optimization and generalization in deep learning. In particular, I am interested in adaptive stochastic gradient descent algorithms, generalization of overparameterized systems, sparse random feature models, and robust learning.\nBefore joining UT Austin, I received my bachelor’s degree in Mathematics and Applied Mathematics from Zhejiang University, China. And I was an exachange student at Lund University, Sweden, during Fall 2015.","tags":null,"title":"Yuege (Gail) Xie","type":"authors"},{"authors":["**Yuege Xie**","Bobby Shi","Hayden Schaeffer","Rachel Ward"],"categories":[],"content":"","date":1638835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638835200,"objectID":"182c6c3ea9e61262e2f714035299dccc","permalink":"/publication/shrimp/","publishdate":"2021-12-07T00:00:00Z","relpermalink":"/publication/shrimp/","section":"publication","summary":" ","tags":[],"title":"SHRIMP: Sparser Random Feature Models via Iterative Magnitude Pruning","type":"publication"},{"authors":["Xiaoxia Wu","**Yuege Xie**","Simon Du","Rachel Ward"],"categories":null,"content":"","date":1631836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631836800,"objectID":"90945e49bf915ae72108e41b85892a38","permalink":"/publication/adaloss/","publishdate":"2021-09-17T00:00:00Z","relpermalink":"/publication/adaloss/","section":"publication","summary":" ","tags":[],"title":"AdaLoss: A computationally-efficient and provably convergent adaptive gradient method","type":"publication"},{"authors":["**Yuege Xie**","Hung-Hsu Chou","Holger Rauhut","Rachel Ward"],"categories":[],"content":"","date":1592179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592179200,"objectID":"35736b4f3effa0e0eacb887cda89b668","permalink":"/publication/weightedopt/","publishdate":"2020-06-15T00:00:00Z","relpermalink":"/publication/weightedopt/","section":"publication","summary":" ","tags":[],"title":"Overparameterization and generalization error: weighted trigonometric interpolation","type":"publication"},{"authors":["**Yuege Xie**","Xiaoxia Wu","Rachel Ward"],"categories":null,"content":"","date":1591142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591142400,"objectID":"ac53ed8f03c65b96f793102e0b089ea9","permalink":"/publication/adalinear/","publishdate":"2020-06-03T00:00:00Z","relpermalink":"/publication/adalinear/","section":"publication","summary":" ","tags":[],"title":"Linear Convergence of Adaptive Stochastic Gradient Descent","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5ab5b5c334d884c20f59e1fb8bdfc46e","permalink":"/post/ds/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/ds/","section":"post","summary":"","tags":null,"title":"Probability","type":"post"}]